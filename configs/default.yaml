# SCoRe Training Configuration
# Override any of these via command-line arguments to scripts/train.py

# Model paths
actor_model: "meta-llama/Llama-3.2-3B-Instruct"
reward_model: "meta-llama/Llama-3.2-3B-Instruct"
reward_lora_path: "/path/to/reward_lora_adapter"
dataset_path: "data/final_dpo_data.json"
output_dir: "outputs/"

# DeepSpeed / memory optimization
offload: false
dtype: "bf16"
actor_zero_stage: 2
reward_zero_stage: 0
offload_reward_model: false
actor_gradient_checkpointing: true

# LoRA configuration
actor_lora_dim: 32                   # Set to 0 to disable LoRA (full fine-tuning)
actor_lora_module_name: "layers."    # Substring to match for LoRA injection
only_optimize_lora: true

# Tokenization / generation
max_prompt_seq_len: 2480
max_answer_seq_len: 1024
max_prompt_len_attempt1: 512         # Max prompt length for Attempt 1
max_new_tokens_attempt1: 1240        # Max new tokens for Attempt 1
max_prompt_len_attempt2: 2480        # Max prompt length for Attempt 2
max_new_tokens_attempt2: 1240        # Max new tokens for Attempt 2
temperature: 0.7

# Training
num_epochs: 3
per_device_training_batch_size: 1
gradient_accumulation_steps: 1
actor_dropout: 0.005

# Learning rate
actor_learning_rate: 5.0e-5
actor_lora_learning_rate: 5.0e-5
actor_weight_decay: 0.01
lr_scheduler_type: "cosine"
num_warmup_steps: 50

# Hybrid engine (enables DeepSpeed inference optimizations during generation)
enable_hybrid_engine: true
inference_tp_size: 2
release_inference_cache: false
unpin_actor_parameters: false
tp_gather_partition_size: 128

# Mixed precision LoRA
enable_mixed_precision_lora: false

# Reproducibility
seed: 42
